{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Simple Linear Regression:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Important function derivation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note on notation:\n",
    "* $x^{(i)}$ is a vector including the feature values for the $i^{th}$ datapoint\n",
    "* $y^{(i)}$ is the label value for the $i^{th}$ datapoint\n",
    "\n",
    "And $X$ is the matrix that includes the transposed feature vectors as such: $$X=\\begin{pmatrix}(x^{(1)})^{T} \\\\ (x^{(2)})^{T} \\\\ . \\\\ . \\\\ . \\\\ (x^{(m)})^{T}\\end{pmatrix}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Hypothesis Function**:\n",
    "$$H(X)=\\theta_{0}+\\theta_{1} x^{1}$$\n",
    "\n",
    "$\\theta_{0}$ being the cut and $\\theta_{1}$ being the slope"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Cost (Squared Error) Function:**\n",
    "$$J(\\theta_{0},\\theta_{1})=\\frac{1}{2m}\\sum^{m}_{i=1}[H(x^{(i)})-y^{(i)}]^2$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Gradient Descent:**\n",
    "$$T_{i} := \\theta_{i} - \\alpha\\frac{\\partial}{\\partial\\theta_{i}}J(\\theta_{0},\\theta_{1})$$\n",
    "\n",
    "$$T_{i} := \\theta_{i} - \\alpha\\frac{\\partial}{\\partial\\theta_{i}}\\frac{1}{2m}\\sum^{m}_{i=1}[H(X_{i})-Y_{i}]^2$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Taking partial derivatives using the simple Linear regression:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\\frac{\\partial}{\\partial\\theta_{i}}\\frac{1}{2m}\\sum^{m}_{i=1}[\\theta_{0}+\\theta_{1}X-Y_{i}]^2$$\n",
    "\n",
    "for $\\theta_{0}$\n",
    "\n",
    "$$\\frac{\\partial}{\\partial\\theta_{0}}\\frac{1}{2m}\\sum^{m}_{i=1}[\\theta_{0}+\\theta_{1}X-Y_{i}]^2 = \\frac{1}{m}\\sum^{m}_{i=1}[\\theta_{0}+\\theta_{1}X-Y_{i}]^2 = \\frac{1}{m}\\sum^{m}_{i=1}[H(X_{i})-Y_{i}]^2$$\n",
    "\n",
    "for $\\theta_{1}$\n",
    "\n",
    "$$\\frac{\\partial}{\\partial\\theta_{1}}\\frac{1}{2m}\\sum^{m}_{i=1}[\\theta_{0}+\\theta_{1}X-Y_{i}]^2 = \\frac{1}{m}\\sum^{m}_{i=1}[\\theta_{0}+\\theta_{1}X-Y_{i}]^2X = \\frac{1}{m}\\sum^{m}_{i=1}[H(X_{i})-Y_{i}]^2X$$\n",
    "\n",
    "**Therefore:** Simultaneous updating of the dynamic functions\n",
    "\n",
    "$$T_{0} := \\theta_{0} - \\alpha\\frac{1}{m}\\sum^{m}_{i=1}[H(X_{i})-Y_{i}]^2 \\longrightarrow \\theta_{0} := T_{0}$$\n",
    "\n",
    "$$T_{1} := \\theta_{1} - \\alpha\\frac{1}{m}\\sum^{m}_{i=1}[H(X_{i})-Y_{i}]^2X \\longrightarrow \\theta_{1} := T_{1}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import pdb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.chdir(r'C:\\Users\\andre.bravo\\Documents\\python\\Python-Data-Science-and-Machine-Learning-Bootcamp\\ML from Scratch\\1. Simple Linear Regression\\Datasets')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = pd.read_csv('Housing_Data.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#first column is indepndent variable X\n",
    "indepX = dataset.iloc[:,0].values\n",
    "#second column is dependent variable Y\n",
    "depY = dataset.iloc[:,1].values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train-Test Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "indepX_train, indepX_test, depY_train, depY_test = train_test_split(indepX,\n",
    "                                                                   depY,\n",
    "                                                                   test_size=0.2,\n",
    "                                                                   random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Main Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main():\n",
    "    \n",
    "    # Initial values\n",
    "    init_theta = np.zeros(2) #creates array, (0,0)\n",
    "    learn_rate = 0.05\n",
    "    num_iterat = 10 # may change later\n",
    "    #hypothesis vector, loop over an entire vector where we assign values as 0\n",
    "    H = [0 for i in range(len(indepX_train))]\n",
    "    \n",
    "    #call functions that will run algorithm\n",
    "    theta = gradientDescent(indepX_train,\n",
    "                            depY_train,\n",
    "                            init_theta,\n",
    "                            learn_rate,\n",
    "                            num_iterat)\n",
    "    H = hyp(theta, indepX_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hypothesis Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def hyp(theta, x):\n",
    "    return theta[0] + theta[1] * x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gradient Descent Function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Two looping processes:\n",
    "1. Loop over entire dataset to calculate loss (or gradient) then adjust our parameters\n",
    "2. Loops the first loop several times until optimal parameters are reached (iteration of the algorithm)\n",
    "Split these two operations over two different functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gradientDescent(indepX, depY, init_theta, learn_rate, num_iterat):\n",
    "    #initialization\n",
    "    theta = init_theta\n",
    "    for i in range(num_iterat):\n",
    "        theta = grad(indepX, depY, theta, learn_rate)\n",
    "        \n",
    "    \n",
    "    \n",
    "    return theta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def grad(indepX, depY, curr_theta, learning_rate):\n",
    "    #initialization\n",
    "    grad = np.zeros(2)\n",
    "    new_theta = curr_theta\n",
    "    m = len(indepX)\n",
    "    \n",
    "    #Loop for adjustment and calculating the gradient\n",
    "    #pdb.set_trace()\n",
    "    for i in range(m):\n",
    "        x = indepX[i]\n",
    "        y = depY[i]\n",
    "        \n",
    "        # Note that summation is acchieved through \"grad[0] + \" which is \"+=\"\n",
    "        grad[0] += (1/m) * ((curr_theta[0] + curr_theta[1] * x) - y)\n",
    "        grad[1] += (1/m) * ((curr_theta[0] + curr_theta[1] * x) - y) * x\n",
    "    \n",
    "    #begin assignment using temporary values\n",
    "    temp0 = curr_theta[0] - (learning_rate * grad[0])\n",
    "    temp1 = curr_theta[1] - (learning_rate * grad[1])\n",
    "    \n",
    "    new_theta[0] = temp0\n",
    "    new_theta[1] = temp1\n",
    "    \n",
    "    return new_theta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main():\n",
    "    # Initial values\n",
    "    init_theta = np.zeros(2) #creates array, (0,0)\n",
    "    learn_rate = 0.05\n",
    "    num_iterat = 56\n",
    "    # may change later\n",
    "    #hypothesis vector, loop over an entire vector where we assign values as 0\n",
    "    H = [0 for i in range(len(indepX_train))]\n",
    "    \n",
    "    #call functions that will run algorithm\n",
    "    theta = gradientDescent(indepX_train, depY_train, init_theta, learn_rate, num_iterat)\n",
    "    \n",
    "    H = hyp(theta, indepX_train)\n",
    "    for i in range(len(depY_test)):\n",
    "        print(float(H[i]))\n",
    "        print(depY_test[i])\n",
    "        print('---------------------')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-inf\n",
      "118525\n",
      "---------------------\n",
      "-inf\n",
      "332981\n",
      "---------------------\n",
      "-inf\n",
      "260935\n",
      "---------------------\n",
      "-inf\n",
      "402892\n",
      "---------------------\n",
      "-inf\n",
      "157694\n",
      "---------------------\n",
      "-inf\n",
      "439039\n",
      "---------------------\n",
      "-inf\n",
      "189318\n",
      "---------------------\n",
      "-inf\n",
      "183504\n",
      "---------------------\n",
      "-inf\n",
      "282501\n",
      "---------------------\n",
      "-inf\n",
      "159783\n",
      "---------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\andre.bravo\\documents\\python\\python-data-science-and-machine-learning-bootcamp\\pyboot\\lib\\site-packages\\ipykernel_launcher.py:2: RuntimeWarning: overflow encountered in multiply\n",
      "  \n"
     ]
    }
   ],
   "source": [
    "if __name__ == '__main__':\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PyBoot",
   "language": "python",
   "name": "pyboot"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
